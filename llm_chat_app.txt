FE should have following components
1. Chat component:
    - Text field for chat history
    - Text box where user can type mesages
    - Send button next to the text box
    - Attach file button next to the send (User can attach pdf file to the message)
2. Model selection component
    - Dropdown for different models which user can select at any time
    - Text field next or below this dropown that says which model is loaded on the server
    - Spinner that can appear next to the current model text field
4. Query model parameters component (For later)
    - To be defined
5. Rag selection and manipulation component (For later)
    - Dropdown with all rag databases that user can select at any time
    - Add files to rag button
    - Dont know what this is but user should be able to attach multiple files

FE should have following functionality
1. Initial connection
    - On startup client connects to ws on the server. All comunication is done through this websocket.
    - After ws connection is established client receives server state json which helps in filling out model dropdown as well as current model and spinner
2. State update functionality
    - At any point client can receive server state update json. This json is used for changing current model field as well as enabling or disabling spinner
3. Chat functionality and behaviour
    - User fills the message, clicks send, query request json is sent to the server over websocket.
    - When send is pressed we should check if there is file attached, and if there is we should base64 encode it and add it to the query request json.
    - When send is pressed it should stay disabled onward. Nothing else should be disabled.
      The message should be displayed in chat history text box at this point.
    - Server sends to user query update jsons which should be displayed somewhere. There could be only one or there could be multiple.
    - Server sends query response jsons in chunk updates untill laste update. Think of this as query response text split into words and each word is one query response json.
      Last word is the end of the reply. These words should be displayed in chat history text field as they come.
    - When last query response json is received send button should be re enabled
4. Query model parameters functionality (For later)
    - Nothing hppens when we change these
5. Rag selection and manipulation functionality (For later)
    - To be defined
6. Server heartbeat
    - Server sends heartbeat messages every 20 seconds, client can discard these

JSON definitions:

server state json (Received from server when client connects first time to websocket):
{
    "type":"server_state",
    "message":
    {
        "llm_state":"READY",            //Indicates state of the server. It can be either READY or CHANGING_MODEL
                                    //If CHANGING_MODEL, then the spinner should appear next to the current model textbox (gayed out?)
        "available_models":         //Array of available model names, used for creating model selection dropdown
        [
            "model1",
            "model2"
        ],
        "current_model":"model2"    //Model that is currently loaded, used for loaded model textbox as well as initial selection in the dropdown
        "current_active_clients":5  //Number of current active clients
    }
}

server state update json (Can be received any time):
{
    "type":"server_state_update",
    "message":                                   //Model change started example
    {
        "update_type":"model_change",            //Either model change -> updates on the progess of model change, either it has started or finished
        "state":"CHANGING_MODEL",                //Spinner should appear next to current model text box, also currrent model text box should chang to current_model
        "current_model":"model1"                 //And maybe be grayed out
    }
    or
    "message":                                   //Model change finished example
    {
        "update_type":"model_change",            //Either model change -> updates on the progess of model change, either it has started or finished
        "state":"READY",                         //Spinner should disappear next to current model text box
        "current_model":"model1"                 //Probbly redundant
    }
    or
    "message":                                   //Number of active clients changed
    {
        "update_type":"active_client_update",    
        "current_active_clients":4              
    }
}

query request json (client sends these to server when send button is pressed):
{
    "type":"query_request",
    "message":
    {
        "query":"Some text",                        //Text from chat text field
        "model":"Selected model from dropdown",     //Model that is selected in dropdown
        "parameters":{},                            //To be defined
        "file_data":"base64 string",                //If file is attached this should be present and filled with base64 encoded file data
                                                    //If file is not attached this field shouldnt exist
    }
}

query update json (Received from server after query is sent):
{
    "type":"query_request_update",
    "message":
    {
        "update":"Some text",       //Some text that should be displayed somewhere
    }
}

query response json (Received from server after query update jsons):
{
    "type":"query_response",
    "message":
    {
        "token":"Some text",           //Message chunk that should be displayed
        "last_token":false              //Indicates if this is the last chunk of the message or not, when this is true the send button gets re enabled
    }
}
server heartbeat json (Can be received any time, can be discarded):
{
    "type":"heartbeat",
    "message":
    {
        "server_time":"28/05/2024 00:58:00" // for example
    }
}

- Python client can accept 3 commands, query (to send query), file(to attach file), close(to close connection), commads are given by typing command the pressing enter)
  In following tests the commands sent to client are denoted with $<command>
Communication tests:

1. Test initial connection:
    Test 1.1 - Test initial connection when server is ready (connect client after server loads the model), it shoud reply with state=READY:
    Reply from server: {"type":"server_state","message":{"available_models": ["mixtral-8x7b-instruct-v0.1.Q4_K_M", "mixtral-8x7b-instruct-v0.1.Q5_K_M", "Meta-Llama-3-8B-Instruct.Q5_K_M", "Meta-Llama-3-8B-Instruct.Original", "Meta-Llama-3-70B-Instruct.Q5_K_M"], "current_model": "mixtral-8x7b-instruct-v0.1.Q4_K_M", "llm_state": "READY", "current_active_clients": 1}}

2. Test request queries and replies:

Test 2.1 - Testing simple query wthout file and no model change:
Request:    $query                                                      // input query command then press enter to input query 
            ${"type":"query_request","message":{"query":"Hi, this is the query.","model":"mixtral-8x7b-instruct-v0.1.Q4_K_M","parameters":{}}}
Reply:      {"type":"query_request_update","message":{"update":"Current position in queue: 1"}}                                                     // Only one client is connected => it is put into a queue
            {"type":"query_request_update","message":{"update":"Your request is being executed"}}                                                   // But it is taken out of the queue immediately.                                              
            {"type":"query_response","message":{"token":"Your","last_token":false}}                                                                 // You start receiving reply word by word, "last_token":false indicating there will be more words comming
            {"type":"query_response","message":{"token":"query","last_token":false}}                                                                // When you collect all words it says at the end "You didnt provide file with query"
            {"type":"query_response","message":{"token":"was:","last_token":false}}
            {"type":"query_response","message":{"token":"Hi,","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"is","last_token":false}}
            {"type":"query_response","message":{"token":"the","last_token":false}}
            {"type":"query_response","message":{"token":"query.;","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"wanted","last_token":false}}
            {"type":"query_response","message":{"token":"to","last_token":false}}
            {"type":"query_response","message":{"token":"run","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"on","last_token":false}}
            {"type":"query_response","message":{"token":"mixtral-8x7b-instruct-v0.1.Q4_K_M","last_token":false}}
            {"type":"query_response","message":{"token":"model.","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"didnt","last_token":false}}
            {"type":"query_response","message":{"token":"provide","last_token":false}}
            {"type":"query_response","message":{"token":"file","last_token":false}}
            {"type":"query_response","message":{"token":"with","last_token":false}}
            {"type":"query_response","message":{"token":"query.","last_token":true}}                                                                // Message is complete, indicated by "last_token":true

Test 2.2 - Testing simple query with file and no model change:
Request:    $file                                                       // Input file command for clinet and press enter to attach file
            $some_file.pdf                                              // Provide path to pdf file then press enter
            $query                                                      // input query command then press enter to input query
            ${"type":"query_request","message":{"query":"Hi, this is the query.","model":"mixtral-8x7b-instruct-v0.1.Q4_K_M","parameters":{}}}      // Client will add "file_data":"base64 string" field to json with file data converted to base64 string
Reply:      {"type":"query_request_update","message":{"update":"Current position in queue: 1"}}
            {"type":"query_request_update","message":{"update":"Your request is being executed"}}
            {"type":"query_response","message":{"token":"Your","last_token":false}}                                                                                         // When you collect all words it says at the end "You provided file with query"
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"was:","last_token":false}}
            {"type":"query_response","message":{"token":"Hi,","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"is","last_token":false}}
            {"type":"query_response","message":{"token":"the","last_token":false}}
            {"type":"query_response","message":{"token":"query.;","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"wanted","last_token":false}}
            {"type":"query_response","message":{"token":"to","last_token":false}}
            {"type":"query_response","message":{"token":"run","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"on","last_token":false}}
            {"type":"query_response","message":{"token":"mixtral-8x7b-instruct-v0.1.Q4_K_M","last_token":false}}
            {"type":"query_response","message":{"token":"model.","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"provided","last_token":false}}
            {"type":"query_response","message":{"token":"file","last_token":false}}
            {"type":"query_response","message":{"token":"with","last_token":false}}
            {"type":"query_response","message":{"token":"query.","last_token":true}}

Test 2.3 - Testing simple query with no file and model change:
Request:    $query                                                      // input query command then press enter to input query 
            ${"type":"query_request","message":{"query":"Hi, this is the query.","model":"Meta-Llama-3-70B-Instruct.Q5_K_M","parameters":{}}}                        // Now we put different model from the the one that is loaded
Reply:      {"type":"query_request_update","message":{"update":"Current position in queue: 1"}}
            {"type":"query_request_update","message":{"update":"Your request is being executed"}}
            {"type":"server_state_update","message":{"update_type":"model_change", "state":"CHANGING_MODEL", "current_model":"Meta-Llama-3-70B-Instruct.Q5_K_M"}}   // You receive this message indicating that server will start changing model, you triggered this with your request but any client can trigger it and you will receive it
            {"type":"server_state_update","message":{"update_type":"model_change", "state":"READY", "current_model":"Meta-Llama-3-70B-Instruct.Q5_K_M"}}            // After 10s you will receive update that model changing has finished       
            {"type":"query_response","message":{"token":"Your","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"was:","last_token":false}}
            {"type":"query_response","message":{"token":"Hi,","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"is","last_token":false}}
            {"type":"query_response","message":{"token":"the","last_token":false}}
            {"type":"query_response","message":{"token":"query.;","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"wanted","last_token":false}}
            {"type":"query_response","message":{"token":"to","last_token":false}}
            {"type":"query_response","message":{"token":"run","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"on","last_token":false}}
            {"type":"query_response","message":{"token":"Meta-Llama-3-70B-Instruct.Q5_K_M","last_token":false}}
            {"type":"query_response","message":{"token":"model.","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"didnt","last_token":false}}
            {"type":"query_response","message":{"token":"provide","last_token":false}}
            {"type":"query_response","message":{"token":"file","last_token":false}}
            {"type":"query_response","message":{"token":"with","last_token":false}}
            {"type":"query_response","message":{"token":"query.","last_token":true}}
You can send same request again and now you wont receive query_request_update jsons because model loaded on the server is the one you are querying

Test 2.4 - Testing simple query with both file and model change:
Request:    $file                                                       // Input file command for clinet and press enter to attach file
            $some_file.pdf                                              // Provide path to pdf file then press enter
            $query                                                      // input query command then press enter to input query 
            ${"type":"query_request","message":{"query":"Hi, this is the query.","model":"Meta-Llama-3-8B-Instruct.Original","parameters":{}}}   // Now we put different model from the the one that is loaded and also include file data
Reply:      {"type":"query_request_update","message":{"update":"Current position in queue: 1"}}
            {"type":"query_request_update","message":{"update":"Your request is being executed"}}
            {"type":"server_state_update","message":{"update_type":"model_change", "state":"CHANGING_MODEL", "current_model":"Meta-Llama-3-8B-Instruct.Original"}}          // You now receive model change jsons and also when you collect all words it says at the end "You didnt provide file with query"
            {"type":"server_state_update","message":{"update_type":"model_change", "state":"READY", "current_model":"Meta-Llama-3-8B-Instruct.Original"}}
            {"type":"query_response","message":{"token":"Your","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"was:","last_token":false}}
            {"type":"query_response","message":{"token":"Hi,","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"is","last_token":false}}
            {"type":"query_response","message":{"token":"the","last_token":false}}
            {"type":"query_response","message":{"token":"query.;","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"wanted","last_token":false}}
            {"type":"query_response","message":{"token":"to","last_token":false}}
            {"type":"query_response","message":{"token":"run","last_token":false}}
            {"type":"query_response","message":{"token":"this","last_token":false}}
            {"type":"query_response","message":{"token":"query","last_token":false}}
            {"type":"query_response","message":{"token":"on","last_token":false}}
            {"type":"query_response","message":{"token":"Meta-Llama-3-8B-Instruct.Original","last_token":false}}
            {"type":"query_response","message":{"token":"model.","last_token":false}}
            {"type":"query_response","message":{"token":"You","last_token":false}}
            {"type":"query_response","message":{"token":"provided","last_token":false}}
            {"type":"query_response","message":{"token":"file","last_token":false}}
            {"type":"query_response","message":{"token":"with","last_token":false}}
            {"type":"query_response","message":{"token":"query.","last_token":true}}


Multi client tests:


Instructions for running server sim on Linux (open terminal and position inside main folder):
1. Set up environment (Only done once):
    pip install python                      # If dont have python and you are using ubuntu, if using other distro check online how to install
    python -m venv venv
    source venv/bin/activate
    pip install -r requirements-sim.txt
2. To run server (in simulation mode):
    source venv/bin/activate                # Only do this if it was not done already in this terminal
    python ./server.py -s -du
3. To run the python test client (You can run multiple clients):
    source venv/bin/activate                # Only do this if it was not done already in this terminal
    python ./test_client.py.py -t           # You can paste json examples from this file into client to see how it behaves

Instructions for running server sim on windows (open power shell from the project main folder):
- You need to install python on windows
1. Set up environment (Only done once):
    python -m venv venv                     # Maybe use python.exe instead of python if it is not put properly on the path
    .\venv\Scripts\Activate.ps1             # If there is a problem with priviledges while running this try running command: Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
    pip install -r .\requirements-sim.txt
2. To run server:
    .\venv\Scripts\Activate.ps1             # Only do this if it was not done already in this power shell window
    python .\server.py -s -du               # You can run python .\server.py -h to see what parameters can be set, for example adding -t 0.1 will lower delay between words sent to client to 0.1 seconds
3. To run the python test client (You can run multiple clients):
    .\venv\Scripts\Activate.ps1             # Only do this if it was not done already in this power shell window
    python .\test_client.py.py -t           # You can paste json examples from this file into client to see how it behaves